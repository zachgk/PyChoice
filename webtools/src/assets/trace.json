{
  "items": [
    {
      "func": "554e5e2a-dae9-54f7-a922-ef9bc49b3568",
      "impl": "b5bf22ed-966e-5d27-9840-d4d349feea56",
      "rules": [
        {
          "rule": {
            "selector": "main CrossEntropyLoss",
            "impl": "b5bf22ed-966e-5d27-9840-d4d349feea56"
          },
          "captures": {},
          "vals": {}
        }
      ],
      "stack_info": [
        "__call__ at /data/projects/PyChoice/pychoice/funcs.py:168",
        "__init__ at /data/projects/dlchoice/dlchoice/module.py:12",
        "main at /data/projects/dlchoice/dlchoice/image_classification.py:24",
        "<module> at /data/projects/dlchoice/dlchoice/image_classification.py:35"
      ],
      "args": [],
      "kwargs": {},
      "choice_kwargs": {},
      "items": []
    },
    {
      "func": "94f53060-633e-5e29-8783-8dce0e0b1ae9",
      "impl": "94f53060-633e-5e29-8783-8dce0e0b1ae9",
      "rules": [],
      "stack_info": [
        "__call__ at /data/projects/PyChoice/pychoice/funcs.py:168",
        "main at /data/projects/dlchoice/dlchoice/image_classification.py:26",
        "<module> at /data/projects/dlchoice/dlchoice/image_classification.py:35"
      ],
      "args": [
        "ChoiceModule(\n  (model): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act1): ReLU(inplace=True)\n        (aa): Identity()\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act2): ReLU(inplace=True)\n      )\n    )\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (fc): Linear(in_features=512, out_features=10, bias=True)\n  )\n  (loss_fn): CrossEntropyLoss()\n)",
        "{Train dataloader: size=60000}\n{Validation dataloader: None}\n{Test dataloader: None}\n{Predict dataloader: None}"
      ],
      "kwargs": {},
      "choice_kwargs": {},
      "items": [
        {
          "func": "1404ba02-c74f-5c0d-97f2-63c458a9f7b3",
          "impl": "1404ba02-c74f-5c0d-97f2-63c458a9f7b3",
          "rules": [
            {
              "rule": {
                "selector": "main ChoiceTrainer",
                "impl": "1404ba02-c74f-5c0d-97f2-63c458a9f7b3"
              },
              "captures": {},
              "vals": {
                "max_steps": "10"
              }
            }
          ],
          "stack_info": [
            "__call__ at /data/projects/PyChoice/pychoice/funcs.py:168",
            "train at /data/projects/dlchoice/dlchoice/train.py:152",
            "__call__ at /data/projects/PyChoice/pychoice/funcs.py:174",
            "main at /data/projects/dlchoice/dlchoice/image_classification.py:26",
            "<module> at /data/projects/dlchoice/dlchoice/image_classification.py:35"
          ],
          "args": [],
          "kwargs": {},
          "choice_kwargs": {
            "max_steps": "10"
          },
          "items": []
        },
        {
          "func": "6e5a2c2e-7ebc-5990-b9fc-d4cf889a79c0",
          "impl": "e686c7dc-9940-52c6-b820-79ec6930339a",
          "rules": [
            {
              "rule": {
                "selector": " AdamW",
                "impl": "e686c7dc-9940-52c6-b820-79ec6930339a"
              },
              "captures": {},
              "vals": {}
            },
            {
              "rule": {
                "selector": "main AdamW",
                "impl": "e686c7dc-9940-52c6-b820-79ec6930339a"
              },
              "captures": {},
              "vals": {
                "lr": "0.005"
              }
            }
          ],
          "stack_info": [
            "__call__ at /data/projects/PyChoice/pychoice/funcs.py:168",
            "configure_optimizers at /data/projects/dlchoice/dlchoice/module.py:22",
            "_call_lightning_module_hook at /data/projects/dlchoice/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:177",
            "_init_optimizers_and_lr_schedulers at /data/projects/dlchoice/.venv/lib/python3.13/site-packages/lightning/pytorch/core/optimizer.py:180",
            "setup_optimizers at /data/projects/dlchoice/.venv/lib/python3.13/site-packages/lightning/pytorch/strategies/strategy.py:139",
            "setup at /data/projects/dlchoice/.venv/lib/python3.13/site-packages/lightning/pytorch/strategies/strategy.py:159",
            "_run at /data/projects/dlchoice/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:988",
            "_fit_impl at /data/projects/dlchoice/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:599",
            "_call_and_handle_interrupt at /data/projects/dlchoice/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:49",
            "fit at /data/projects/dlchoice/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:561",
            "train at /data/projects/dlchoice/dlchoice/train.py:153",
            "__call__ at /data/projects/PyChoice/pychoice/funcs.py:174",
            "main at /data/projects/dlchoice/dlchoice/image_classification.py:26",
            "<module> at /data/projects/dlchoice/dlchoice/image_classification.py:35"
          ],
          "args": [
            "<generator object Module.parameters at 0x7fde40b12960>"
          ],
          "kwargs": {},
          "choice_kwargs": {
            "lr": "0.005"
          },
          "items": []
        }
      ]
    }
  ],
  "registry": {
    "554e5e2a-dae9-54f7-a922-ef9bc49b3568": {
      "id": "554e5e2a-dae9-54f7-a922-ef9bc49b3568",
      "interface": {
        "id": "554e5e2a-dae9-54f7-a922-ef9bc49b3568",
        "func": "loss_function",
        "module": "dlchoice.loss",
        "defaults": {},
        "doc": null
      },
      "funcs": {
        "b5bf22ed-966e-5d27-9840-d4d349feea56": {
          "id": "b5bf22ed-966e-5d27-9840-d4d349feea56",
          "func": "CrossEntropyLoss",
          "module": "torch.nn.modules.loss",
          "defaults": {
            "weight": "None",
            "size_average": "None",
            "ignore_index": "-100",
            "reduce": "None",
            "reduction": "mean",
            "label_smoothing": "0.0"
          },
          "doc": "This criterion computes the cross entropy loss between input logits\nand target.\n\nIt is useful when training a classification problem with `C` classes.\nIf provided, the optional argument :attr:`weight` should be a 1D `Tensor`\nassigning weight to each of the classes.\nThis is particularly useful when you have an unbalanced training set.\n\nThe `input` is expected to contain the unnormalized logits for each class (which do `not` need\nto be positive or sum to 1, in general).\n`input` has to be a Tensor of size :math:`(C)` for unbatched input,\n:math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n`K`-dimensional case. The last being useful for higher dimension inputs, such\nas computing cross entropy loss per-pixel for 2D images.\n\nThe `target` that this criterion expects should contain either:\n\n- Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\n  `ignore_index` is specified, this loss also accepts this class index (this index\n  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n  set to ``'none'``) loss for this case can be described as:\n\n  .. math::\n      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n\n  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n\n  .. math::\n      \\ell(x, y) = \\begin{cases}\n          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n           \\text{if reduction} = \\text{`mean';}\\\\\n            \\sum_{n=1}^N l_n,  &\n            \\text{if reduction} = \\text{`sum'.}\n        \\end{cases}\n\n  Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`\n  on an input, followed by :class:`~torch.nn.NLLLoss`.\n\n- Probabilities for each class; useful when labels beyond a single class per minibatch item\n  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n\n  .. math::\n      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\n\n  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n\n  .. math::\n      \\ell(x, y) = \\begin{cases}\n          \\frac{\\sum_{n=1}^N l_n}{N}, &\n           \\text{if reduction} = \\text{`mean';}\\\\\n            \\sum_{n=1}^N l_n,  &\n            \\text{if reduction} = \\text{`sum'.}\n        \\end{cases}\n\n.. note::\n    The performance of this criterion is generally better when `target` contains class\n    indices, as this allows for optimized computation. Consider providing `target` as\n    class probabilities only when a single class label per minibatch item is too restrictive.\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to each class.\n        If given, has to be a Tensor of size `C`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    ignore_index (int, optional): Specifies a target value that is ignored\n        and does not contribute to the input gradient. When :attr:`size_average` is\n        ``True``, the loss is averaged over non-ignored targets. Note that\n        :attr:`ignore_index` is only applicable when the target contains class indices.\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n        be applied, ``'mean'``: the weighted mean of the output is taken,\n        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in\n        the meantime, specifying either of those two args will override\n        :attr:`reduction`. Default: ``'mean'``\n    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n        become a mixture of the original ground truth and a uniform distribution as described in\n        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n\nShape:\n    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of `K`-dimensional loss.\n    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`. The\n      target data type is required to be long when using class indices. If containing class probabilities, the\n      target must be the same shape input, and each value should be between :math:`[0, 1]`. This means the target\n      data type is required to be float when using class probabilities.\n    - Output: If reduction is 'none', shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n\n\n    where:\n\n    .. math::\n        \\begin{aligned}\n            C ={} & \\text{number of classes} \\\\\n            N ={} & \\text{batch size} \\\\\n        \\end{aligned}\n\nExamples:\n\n    >>> # Example of target with class indices\n    >>> loss = nn.CrossEntropyLoss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n    >>> output = loss(input, target)\n    >>> output.backward()\n    >>>\n    >>> # Example of target with class probabilities\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5).softmax(dim=1)\n    >>> output = loss(input, target)\n    >>> output.backward()"
        }
      },
      "rules": [
        {
          "selector": "main CrossEntropyLoss",
          "impl": "b5bf22ed-966e-5d27-9840-d4d349feea56"
        }
      ]
    },
    "6e5a2c2e-7ebc-5990-b9fc-d4cf889a79c0": {
      "id": "6e5a2c2e-7ebc-5990-b9fc-d4cf889a79c0",
      "interface": {
        "id": "6e5a2c2e-7ebc-5990-b9fc-d4cf889a79c0",
        "func": "create_optimizer",
        "module": "dlchoice.optim",
        "defaults": {
          "parameters": "<class 'inspect._empty'>"
        },
        "doc": null
      },
      "funcs": {
        "e686c7dc-9940-52c6-b820-79ec6930339a": {
          "id": "e686c7dc-9940-52c6-b820-79ec6930339a",
          "func": "AdamW",
          "module": "torch.optim.adamw",
          "defaults": {
            "params": "<class 'inspect._empty'>",
            "lr": "0.001",
            "betas": "(0.9, 0.999)",
            "eps": "1e-08",
            "weight_decay": "0.01",
            "amsgrad": "False"
          },
          "doc": "Implements AdamW algorithm, where weight decay does not accumulate in the momentum nor variance.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{(lr)}, \\: \\beta_1, \\beta_2\n            \\text{(betas)}, \\: \\theta_0 \\text{(params)}, \\: f(\\theta) \\text{(objective)},\n            \\: \\epsilon \\text{ (epsilon)}                                                    \\\\\n        &\\hspace{13mm}      \\lambda \\text{(weight decay)},  \\: \\textit{amsgrad},\n            \\: \\textit{maximize}                                                             \\\\\n        &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ (first moment)}, v_0 \\leftarrow 0\n            \\text{ ( second moment)}, \\: v_0^{max}\\leftarrow 0                        \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}         \\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n        &\\hspace{10mm} v_t^{max} \\leftarrow \\mathrm{max}(v_{t-1}^{max},v_t)                  \\\\\n        &\\hspace{10mm}\\widehat{v_t} \\leftarrow v_t^{max}/\\big(1-\\beta_2^t \\big)              \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                  \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Decoupled Weight Decay Regularization`_.\n\nArgs:\n    params (iterable): iterable of parameters or named_parameters to optimize\n        or iterable of dicts defining parameter groups. When using named_parameters,\n        all parameters in all groups should be named\n    lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\n        is not yet supported for all our implementations. Please use a float\n        LR if you are not also specifying fused=True or capturable=True.\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n    amsgrad (bool, optional): whether to use the AMSGrad variant of this\n        algorithm from the paper `On the Convergence of Adam and Beyond`_\n        (default: False)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a graph, whether for CUDA graphs or for torch.compile support.\n        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n        Passing True can impair ungraphed performance, so if you don't intend to graph\n        capture this instance, leave it False (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None)\n\n.. note:: The foreach and fused implementations are typically faster than the for-loop,\n          single-tensor implementation, with fused being theoretically fastest with both\n          vertical and horizontal fusion. As such, if the user has not specified either\n          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n          implementation is relatively new, we want to give it sufficient bake-in time.\n          To specify fused, pass True for fused. To force running the for-loop\n          implementation, pass False for either foreach or fused. \n.. Note::\n    A prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\n.. _Decoupled Weight Decay Regularization:\n    https://arxiv.org/abs/1711.05101\n.. _On the Convergence of Adam and Beyond:\n    https://openreview.net/forum?id=ryQu7f-RZ"
        }
      },
      "rules": [
        {
          "selector": " AdamW",
          "impl": "e686c7dc-9940-52c6-b820-79ec6930339a"
        },
        {
          "selector": "main AdamW",
          "impl": "e686c7dc-9940-52c6-b820-79ec6930339a"
        }
      ]
    },
    "1404ba02-c74f-5c0d-97f2-63c458a9f7b3": {
      "id": "1404ba02-c74f-5c0d-97f2-63c458a9f7b3",
      "interface": {
        "id": "1404ba02-c74f-5c0d-97f2-63c458a9f7b3",
        "func": "ChoiceTrainer",
        "module": "dlchoice.train",
        "defaults": {
          "accelerator": "auto",
          "strategy": "auto",
          "devices": "auto",
          "num_nodes": "1",
          "precision": "None",
          "logger": "None",
          "callbacks": "None",
          "fast_dev_run": "False",
          "max_epochs": "None",
          "min_epochs": "None",
          "max_steps": "-1",
          "min_steps": "None",
          "max_time": "None",
          "limit_train_batches": "None",
          "limit_val_batches": "None",
          "limit_test_batches": "None",
          "limit_predict_batches": "None",
          "overfit_batches": "0.0",
          "val_check_interval": "None",
          "check_val_every_n_epoch": "1",
          "num_sanity_val_steps": "None",
          "log_every_n_steps": "None",
          "enable_checkpointing": "None",
          "enable_progress_bar": "None",
          "enable_model_summary": "None",
          "accumulate_grad_batches": "1",
          "gradient_clip_val": "None",
          "gradient_clip_algorithm": "None",
          "deterministic": "None",
          "benchmark": "None",
          "inference_mode": "True",
          "use_distributed_sampler": "True",
          "profiler": "None",
          "detect_anomaly": "False",
          "barebones": "False",
          "plugins": "None",
          "sync_batchnorm": "False",
          "reload_dataloaders_every_n_epochs": "0",
          "default_root_dir": "None",
          "model_registry": "None"
        },
        "doc": null
      },
      "funcs": {},
      "rules": [
        {
          "selector": "main ChoiceTrainer",
          "impl": "1404ba02-c74f-5c0d-97f2-63c458a9f7b3"
        }
      ]
    },
    "94f53060-633e-5e29-8783-8dce0e0b1ae9": {
      "id": "94f53060-633e-5e29-8783-8dce0e0b1ae9",
      "interface": {
        "id": "94f53060-633e-5e29-8783-8dce0e0b1ae9",
        "func": "train",
        "module": "dlchoice.train",
        "defaults": {
          "model": "<class 'inspect._empty'>",
          "data": "<class 'inspect._empty'>"
        },
        "doc": null
      },
      "funcs": {},
      "rules": []
    }
  }
}
